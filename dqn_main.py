#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sat Dec 28 14:47:03 2019

@author: filip

!echo "# DQN_pricing_algorithms" >> README.md
!git init
!git add README.md
!git add cont_bertrand.py
!git add config.py
!git add dqn_main.py
!git add dqn_model.py
!git add agent.py
!git add experience_buffer.py
!git commit -m "Adding the other files as well"
!git remote add origin https://github.com/filipmellgren/DQN_pricing_algorithms.git
!git push -u origin master

Grokking pytorch: https://github.com/Kaixhin/grokking-pytorch/blob/master/README.md?fbclid=IwAR1h7QPpTi8-v7ij6fJvVLRm-weI5HzohiUtIG7CDB0jZGr0cE0lEcNApXE
Cartpole with 4d input on stackoverflow: https://stackoverflow.com/questions/56964657/cartpole-v0-loss-increasing-using-dqn

From the correct directory in the terminal write:
    !tensorboard --logdir runs --host localhost
Then go to:
    http://localhost:6006/
in the browser

"""
# Imports I might need:
#from torch.nn import functional as F
#from torch.utils.data import DataLoader
#import gym
# import experience buffer
#from config import ENV

# TODO: do I have to "turn off" the network somehow?
# TODO: one algorithm thrives, the other sucks. Why is this?
# I know that adding many actions don't increase the time by much. But for now, it results in wrong numbers. 
# Wacky transformation of price might explain why I get weird results

import torch
from torch import nn, optim
import numpy as np
from tensorboardX import SummaryWriter
from dqn_model import Net
import time
from agent import Agent1 as Agent
from experience_buffer import ExperienceBuffer
from config import PARAMS
from config import calc_loss
from cont_bertrand import ContBertrand
from config import args #args includes a few arguments used throughout. TODO: should all arguments be passed via this guy?
from config import ECON_PARAMS
from config import avg_profit_gain
ENV = ContBertrand()
import collections

Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])

GAMMA = PARAMS[1]
BATCH_SIZE = int(PARAMS[1])
REPLAY_SIZE = int(PARAMS[2])
REPLAY_START_SIZE = PARAMS[3]
LEARNING_RATE = PARAMS[4]
SYNC_TARGET_FRAMES = PARAMS[5]
EPSILON_DECAY_LAST_FRAME = PARAMS[6]
EPSILON_START =  PARAMS[7]
EPSILON_FINAL = PARAMS[8]
MEAN_REWARD_BOUND = PARAMS[9]
epsilon= EPSILON_START
nA = PARAMS[10].astype(int)
dO = PARAMS[11].astype(int)

NASH_PROFIT = ECON_PARAMS[6]
NASH_PROFIT = NASH_PROFIT[0] # 0.11278
MONOPOLY_PROFIT = ECON_PARAMS[7] # 0.11576

# PyTorch setup
use_cuda = torch.cuda.is_available() and not args.no_cuda
device = torch.device('cuda' if use_cuda else 'cpu') # CPU when GPU is not available <-> device agnostic
print(device)

torch.manual_seed(args.seed)
if use_cuda:
  torch.cuda.manual_seed(args.seed)

# Neural network model:
net0 = Net(2,nA).to(device) # might need layers to depend on sizes of env: env.observation_space.shape, env.action_space.n).to(device)
print(net0)
net1 = Net(2,nA).to(device)
tgt_net0 = Net(2,nA).to(device) # Prediction target. TODO: do I maybe want just one target to speed up training? Might work because of symmetry and that there might be an objective functon
tgt_net1 = Net(2,nA).to(device)
criterion = nn.MSELoss()
optimizer0 = optim.Adam(net0.parameters(), lr=LEARNING_RATE)
optimizer1 = optim.Adam(net1.parameters(), lr=LEARNING_RATE)
buffer0 = ExperienceBuffer(REPLAY_SIZE) # buffer from which to sample data
buffer1 = ExperienceBuffer(REPLAY_SIZE) # buffer from which to sample data

# Reinforcement learning environment
env = ENV
agent0 = Agent(env, buffer0, net0, tgt_net0, optimizer0)
agent1 = Agent(env, buffer1, net1, tgt_net1, optimizer1)

# Write output statistics to tensorboard
writer = SummaryWriter(comment = "-")

# Initialize variables
env.seed(args.seed)
torch.manual_seed(args.seed)

frame_idx = 0
ts_frame = 0
ts = time.time()
# Training â€“ Main loop
# TODO: repeat training loop multiple times for main results
#state = env.reset()
BETA = 10**-5 # TODO: import this from config
s_next = np.array([0,0,0,0]) # TODO: this is a bit too hacky. reset env
for t in range(1, 200_000):
    frame_idx += 1
   # epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME) # TODO: according to book. Looks wrong
    epsilon = np.exp(-BETA*frame_idx)
    
    s = s_next
    # TODO: have two independent epsilons
    action0 = agent0.act(net0, s[np.array([0,2])], epsilon)
    action1 = agent1.act(net1, s[np.array([0,2])], epsilon)
    action1 = 5 # TODO: temporary
    s_next, reward_n, done, _ = env.step(action0, action1) # TODO somehow new state is 4 dim
        
    exp0 = Experience(s_next[np.array([0,2])], action0, reward_n[0], done, s[np.array([0,2])])
    agent0.exp_buffer.append(exp0)
    exp1 = Experience(s_next[np.array([0,2])], action1, reward_n[1], done, s[np.array([0,2])])
    agent1.exp_buffer.append(exp1)
    
    
    if reward_n is not None: # TODO: now I'm doing this for just one agent
        a = 0
        for agent in [agent0, agent1]:
            reward = reward_n[a]
            agent.total_rewards.append(reward)
            speed = (frame_idx - ts_frame) / (time.time() - ts)
            ts_frame  = frame_idx
            ts = time.time()
            mean_reward = np.mean(agent.total_rewards[-100:]) # TODO: this will probably have to be longer, otherwise it only learns based on noise
            apg = avg_profit_gain(reward)
            #writer.add_scalar("reward_100", mean_reward, frame_idx)
            writer.add_scalar(str(a), apg, frame_idx)
            if agent.best_mean_reward is None or agent.best_mean_reward < mean_reward: # What does this step do? 
                torch.save(agent.net.state_dict(),  "-best.dat")
                if agent.best_mean_reward is not None:
                    print("Best mean reward updated, %.3f: %.3f -> %.3f, model saved" % (a, agent.best_mean_reward, mean_reward))
                agent.best_mean_reward = mean_reward
            if mean_reward > args.reward: # TODO: this is where I check for the convergence criterion
                print("Solved in %d frames!" % frame_idx)
                break # TODO: does it break both loops or just one?
            a += 1
            
    if len(agent0.exp_buffer) < REPLAY_START_SIZE: 
        continue

    for agent in [agent0, agent1]:
        if frame_idx % SYNC_TARGET_FRAMES == 0: # Update target network
            agent.tgt_net.load_state_dict(agent.net.state_dict())
        batch = agent.exp_buffer.sample(BATCH_SIZE)
        agent.optimizer.zero_grad()
        loss_t = calc_loss(batch, agent.net, agent.tgt_net, device = device) # does the agent ever value update? Where?
        loss_t.backward()
        agent.optimizer.step()
    writer.add_scalar(str(a) + "loss", loss_t, frame_idx)
    
writer.close()
# TODO: reset agents
# TODO: change mean_reward to avg_profit_gain AND duration of same optimal action
# 20:25, started a 500k iteration step long journey
# Had ended 22:23
# Lead to non convergence. Increase learnign rate


# 08:56, learning rate 10 times as large -> 09:39 (0.05)
# changed # actions to 50 09:40 -> 10:22. Requires more iterations but also came with less noise
# beta 0.5 * 10**-5, alpha =.05, gamma 0.95 (from 0.99), batch size 128 (from 64) 10:44 -> 13:14. Stays at levels that are too low

# TODO: play agent in stationary environment first and let it solve this problem.
    # Check profit function
    # Check the flow
    # Update how nets are saved
# TODO: Let synchronizatin be closer related to learnng rate